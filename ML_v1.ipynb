{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUcGQn_CQUZp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.What is a parameter?**\n",
        "A parameter is a value that is used in the model training process to define the relationship between the input and output. In machine learning, parameters are learned from the data.\n",
        "\n",
        "#**2.What is correlation?**\n",
        "Correlation is a statistical measure that describes the extent to which two variables are related to each other.\n",
        " It tells us how one variable changes with respect to another.\n",
        "\n",
        " The most common type is Pearson correlation, which ranges from -1 to 1:\n",
        " - Perfect positive correlation (both increase together)\n",
        " - No correlation\n",
        " - Perfect negative correlation (one increases, the other decreases)\n",
        "\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "data = {'height': [150, 160, 170, 180], 'weight': [50, 60, 70, 80]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = df.corr()\n",
        "print(\"correlation matrix:\", correlation)\n",
        "```\n",
        "#**3.What does negative correlation mean?**\n",
        "Negative correlation means that as one variable increases, the other decreases. For example, there is a negative correlation between the amount of exercise and weight gain.\n",
        "\n",
        "#**4.Define Machine Learning. What are the main components in Machine Learning?**\n",
        "Machine Learning (ML) is a subset of AI that allows systems to learn from data and improve over time without explicit programming. The main components of ML are:\n",
        "1. **Data**: The input used for training.\n",
        "2. **Model**: The algorithm that learns patterns from the data.\n",
        "3. **Training**: The process where the model learns from data.\n",
        "4. **Evaluation**: Assessing the model's performance.\n",
        "5. **Optimization**: Tuning the model for better results.\n",
        "\n",
        "#**5.How does loss value help in determining whether the model is good or not?**\n",
        "The loss value (or cost function) quantifies how well the model's predictions align with the actual results. A lower loss indicates a better model.\n",
        "\n",
        "**What are continuous and categorical variables?**\n",
        "- **Continuous variables**: These are numeric values that can take any value within a range (e.g., temperature, height).\n",
        "- **Categorical variables**: These represent discrete categories or labels (e.g., gender, color).\n",
        "\n",
        "#**6.How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "Categorical variables can be handled using techniques like:\n",
        "1. **One-Hot Encoding**: Creating binary columns for each category.\n",
        "2. **Label Encoding**: Assigning each category a unique integer value.\n",
        "3. **Ordinal Encoding**: Encoding categories with a meaningful order.\n",
        "\n",
        "#**7.What do you mean by training and testing a dataset?**\n",
        "- **Training dataset**: The data used to teach the model and fit it to patterns.\n",
        "- **Testing dataset**: The data used to evaluate the model's performance after training.\n",
        "\n",
        "#**8.What is sklearn.preprocessing?**\n",
        "`sklearn.preprocessing` is a module in the scikit-learn library that contains functions for preprocessing data, such as scaling, encoding, and transforming features.\n",
        "\n",
        "#**9.What is a Test set?**\n",
        "A test set is a subset of data used to evaluate the model after it has been trained to assess its generalization to new, unseen data.\n",
        "\n",
        "#**10.How do we split data for model fitting (training and testing) in Python?**\n",
        "In Python, we can split data using `train_test_split()` from the `sklearn.model_selection` module:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "#**11.how do you approach a machine learning problem?**\n",
        "- define the problem and objectives.\n",
        "- collect and prepare the data (data cleaning, handling missing values, feature engineering).\n",
        "- perform exploratory data analysis (eda) to understand the data.\n",
        "- select a suitable model.\n",
        "- split the data into training and testing sets.\n",
        "- train the model on the training data using `model.fit()`.\n",
        "- evaluate the model on the testing data using metrics relevant to the problem.\n",
        "- tune hyperparameters to improve performance.\n",
        "- deploy and monitor the model.\n",
        "\n",
        "#**12.why do we have to perform eda before fitting a model to the data?**\n",
        "eda helps us understand the data's characteristics, identify patterns, anomalies, and relationships between variables. it can reveal data quality issues, inform feature engineering, and guide model selection, ultimately leading to a more robust and accurate model.\n",
        "\n",
        "#**13.what is correlation?**\n",
        "correlation is a statistical measure that expresses the extent to which two variables are linearly related. it ranges from -1 to +1, where +1 indicates a perfect positive linear relationship, -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
        "\n",
        "#**14.what does negative correlation mean?**\n",
        "negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa.\n",
        "\n",
        "#**15.how can you find correlation between variables in python?**\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "data = {'feature1': [1, 2, 3, 4, 5], 'feature2': [5, 4, 3, 2, 1], 'feature3': [1, 3, 2, 4, 5]}\n",
        "df = pd.DataFrame(data)\n",
        "correlation_matrix = df.corr()\n",
        "print(f\"correlation matrix:\\n{correlation_matrix}\")\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "corr, p_value = pearsonr(df['feature1'], df['feature2'])\n",
        "print(f\"correlation between feature1 and feature2: {corr}\")\n",
        "```\n",
        "#**16.what is causation? explain difference between correlation and causation with an example.**\n",
        "causation means that one event directly causes another event to occur. correlation indicates an association or relationship between two variables, but it does not necessarily imply that one causes the other.\n",
        "\n",
        "example:\n",
        "- ice cream sales and crime rates might be positively correlated (both tend to increase in the summer). however, this doesn't mean that buying ice cream causes crime, or vice versa. a confounding factor, such as warmer weather, could be influencing both.\n",
        "\n",
        "#**17.what is an optimizer? what are different types of optimizers? explain each with an example.**\n",
        "an optimizer is an algorithm used to adjust the parameters of a machine learning model (e.g., weights and biases in a neural network) during training to minimize the loss function.\n",
        "\n",
        "different types of optimizers:\n",
        " - gradient descent: iteratively moves parameters in the direction of the negative gradient of the loss function. example: basic linear regression training.\n",
        " - stochastic gradient descent (sgd): updates parameters using the gradient of the loss function on a single randomly chosen data point (or a small batch). example: training large neural networks where computing the full gradient is expensive.\n",
        " - adam (adaptive moment estimation): computes adaptive learning rates for each parameter by estimating the first and second moments of the gradients. example: widely used in deep learning for its efficiency and good performance.\n",
        " - rmsprop (root mean square propagation): adapts the learning rates by dividing the gradient by a running average of its recent magnitude. example: effective for recurrent neural networks (rnns).\n",
        " - adagrad (adaptive gradient algorithm): adapts the learning rate for each parameter based on the historical gradients that have been computed for it. example: can be useful for sparse data.\n",
        " example (using tensorflow/keras):\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Dense(1)])\n",
        "optimizer_sgd = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "optimizer_adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer_sgd, loss='mse') # using sgd optimizer\n",
        "model.compile(optimizer=optimizer_adam, loss='mse') # using adam optimizer\n",
        "```\n",
        "#**18.what is sklearn.linear_model ?**\n",
        "`sklearn.linear_model` is a module in the scikit-learn library that implements various linear models for regression and classification tasks. examples include linear regression, logistic regression, ridge regression, lasso, etc.\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model_lr = LinearRegression()\n",
        "```\n",
        "#**19.what does model.fit() do? what arguments must be given?**\n",
        "the model.fit() function in most machine learning frameworks (especially in keras and TensorFlow) is used to train a model on a dataset.\n",
        "- what model.fit() do\n",
        "\n",
        "    it trains the model for a fixed number of epochs (iterations over the full dataset).\n",
        "\n",
        "    it adjusts the model weights using the training data and optimizer.\n",
        "example:\n",
        "```python\n",
        "x_train_lr = np.array([[1], [2], [3]])\n",
        "y_train_lr = np.array([2, 4, 5])\n",
        "model_lr.fit(x_train_lr, y_train_lr)\n",
        "```\n",
        "#**20.what does model.predict() do? what arguments must be given?**\n",
        " model.predict() is used to generate output predictions (inference) from the input samples.\n",
        " It returns the predicted values based on the trained model.\n",
        "\n",
        " Basic syntax:\n",
        " - model.predict(x)\n",
        "\n",
        " Example:\n",
        "\n",
        "```python\n",
        "  import numpy as np\n",
        "\n",
        "  x_test_lr = np.array([[4], [5]])\n",
        "  predictions = model_lr.predict(x_test_lr)\n",
        "  print(f\"predictions: {predictions}\")\n",
        "```\n",
        "\n",
        "#**21.what are continuous and categorical variables?**\n",
        " - continuous variables: can take on any value within a given range (e.g., height, temperature, salary).\n",
        " - categorical variables: represent distinct categories or groups (e.g., color, gender, city).\n",
        "example:\n",
        "- continuous: temperature (25.5, 30.1, etc.), weight (60.2, 75.8, etc.)\n",
        "- categorical: color ('red', 'blue', 'green'), city ('new york', 'london', 'tokyo')\n",
        "\n",
        "#**22.what is feature scaling? how does it help in machine learning?**\n",
        "feature scaling is a technique used to standardize or normalize the range of independent variables (features). it helps machine learning algorithms in several ways:\n",
        " - gradient descent converges faster.\n",
        " - algorithms sensitive to feature scales (e.g., k-nearest neighbors, support vector machines) perform better.\n",
        " - it prevents features with larger values from dominating the learning process.\n",
        "\n",
        "#**23.how do we perform scaling in python?**\n",
        " example:\n",
        " scaling is used to normalize features so they have similar ranges.\n",
        " two common types:\n",
        " 1. standard scaling: mean = 0, std = 1\n",
        " 2. min-max scaling: scaled to [0, 1] range\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data_scaling = np.array([[10, 2], [20, 3], [30, 4]])\n",
        "\n",
        "# Standard Scaling\n",
        "scaler_standard = StandardScaler()\n",
        "scaled_data_standard = scaler_standard.fit_transform(data_scaling)\n",
        "print(f\"standard scaled data:\\n{scaled_data_standard}\")\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "scaled_data_minmax = scaler_minmax.fit_transform(data_scaling)\n",
        "print(f\"min-max scaled data:\\n{scaled_data_minmax}\")\n",
        "```\n",
        "#**24.what is sklearn.preprocessing?**\n",
        "`sklearn.preprocessing` is a module in scikit-learn that provides various utility functions and transformer classes to change raw feature vectors into a more suitable representation for downstream estimators. this includes scaling, normalization, encoding categorical features, handling missing values, and generating polynomial features.\n",
        "\n",
        "#**25.explain data encoding?**\n",
        "data encoding is the process of converting categorical variables into a numerical format that machine learning models can understand and process. common techniques include:\n",
        " - label encoding: assigning a unique numerical label to each category.\n",
        " - one-hot encoding: creating binary columns for each category, where a '1' indicates the presence of that category and '0' otherwise.\n",
        " - ordinal encoding: assigning numerical labels based on a meaningful order or rank of the categories.\n",
        " example:\n",
        " ```python\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "data_encoding = pd.DataFrame({'color': ['red', 'blue', 'green', 'blue', 'red']})\n",
        "label_encoder = LabelEncoder()\n",
        "data_encoding['color_encoded_label'] = label_encoder.fit_transform(data_encoding['color'])\n",
        "print(f\"label encoded data:\\n{data_encoding}\")\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "color_encoded_onehot = onehot_encoder.fit_transform(data_encoding[['color']])\n",
        "color_encoded_df = pd.DataFrame(color_encoded_onehot, columns=onehot_encoder.get_feature_names_out(['color']))\n",
        "data_encoding_onehot = pd.concat([data_encoding, color_encoded_df], axis=1)\n",
        "print(f\"one-hot encoded data:\\n{data_encoding_onehot}\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "S0iF8vC_QX-e"
      }
    }
  ]
}